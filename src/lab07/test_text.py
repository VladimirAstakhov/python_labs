import pytest
from src.lib.text import normalize, tokenize, count_freq, top_n


@pytest.mark.parametrize(
    "text, expected",
    [
        ("–ü—Ä–ò–≤–ï—Ç\n–ú–∏–†\t", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("—ë–∂–∏–∫, –Å–ª–∫–∞", "–µ–∂–∏–∫, –µ–ª–∫–∞"),
        ("Hello\nWorld", "hello world"),
        ("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"),
        ("", ""),
    ],
)
def test_normalize_basic(text, expected):
    assert normalize(text) == expected


@pytest.mark.parametrize(
    "text, expected_tokens",
    [
        ("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]),
        ("hello,world!!!", ["hello", "world"]),
        ("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]),
        ("2025 –≥–æ–¥", ["2025", "–≥–æ–¥"]),
        ("emoji üòÉ –Ω–µ —Å–ª–æ–≤–æ", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]),
        ("", []),
        ("!!! ??? ###", []),
    ],
)
def test_tokenize_basic(text, expected_tokens):
    assert tokenize(text) == expected_tokens


@pytest.mark.parametrize(
    "tokens, expected_freq",
    [
        (
            ["a", "b", "a", "c", "b", "a"],
            {"a": 3, "b": 2, "c": 1},
        ),
        (
            ["bb", "aa", "bb", "aa", "cc"],
            {"bb": 2, "aa": 2, "cc": 1},
        ),
        (["a", "a", "a"], {"a": 3}),
        (["b", "a"], {"b": 1, "a": 1}),
    ],
)
def test_count_freq_basic(tokens, expected_freq):
    assert count_freq(tokens) == expected_freq


@pytest.mark.parametrize(
    "freq, n, expected_top",
    [
        (
            {"a": 3, "b": 2, "c": 1},
            2,
            [("a", 3), ("b", 2)],
        ),
        (
            {"bb": 2, "aa": 2, "cc": 1},
            2,
            [("aa", 2), ("bb", 2)],
        ),
        ({"b": 2, "a": 2, "c": 1}, 2, [("a", 2), ("b", 2)]),
    ],
)
def test_top_n_basic(freq, n, expected_top):
    assert top_n(freq, n) == expected_top
